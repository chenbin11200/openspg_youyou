{
  // -- base model info
  "nn_name": "Baichuan", //nn_name and nn_version are necessary for detect the corrent invoker or executor
  "nn_version": "7B-chat",
  "nn_model_path": "/model/path/to/Baichuan-7B-Chat", // local model path
  "train_dataset_path": "/data/train/dataset.json", // train dataset path
//   "eval_dataset_path": "/data/eval/dataset-eval.json", // eval dataset path, if you want to do eval
  "output_dir": "/mnt/new_nas/users/youyou/output_temp/sft_1__nolora_2", // trained model output dir
  // -- adapter model info
//   "adapter_name": "YouYou", //set it to a not "default" string value to enable adapter sft
//   "adapter_type": "lora", // adapter type. Don't need it if adapter_name is not set
//   "adapter_config": { // only necessary if adapter_name is set, same as peft LoraConfig args if tyep is 'lora'
//     "r": 8,
//     "lora_alpha": 16,
//     "lora_dropout": 0.05,
//     "bias": "none",
//     "target_modules": ["W_pack", "o_proj"], // this is only an example for BaiChuan lora training
//     "task_type": "CAUSAL_LM"
//   },
//   "qlora_bits_and_bytes_config": { // only necessary if you want to quantinize load model
//     "load_in_4bit": true,
//     "bnb_4bit_compute_dtype": "bfloat16",
//     "bnb_4bit_use_double_quant": true,
//     "bnb_4bit_quant_type": "nf4"
//   }
  //-- start training args
//   "resume_from_checkpoint": "True", // only necessary if you want to resume training from checkpoint
  "trust_remote_code": true,
  "input_max_length": 256, // input max length. Inputs will be cutdown to this length
  //-- start: same as huggingface trainer args
  "per_device_train_batch_size": 1,
  "gradient_accumulation_steps": 1,
  "lr_scheduler_type": "cosine", // adjust learning rate scheduler
  "logging_steps": 20,
  "save_steps": 10000,
  "learning_rate": 4e-5,
  "num_train_epochs": 1.0
  //-- end: huggingface trainer args
}